import os
import random
import shutil
import textwrap
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# === TERMINAL COLOR STYLING ===
RESET = '\033[0m'
BOLD = '\033[1m'
RED = '\033[91m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
WHITE = '\033[97m'

# === USER INPUT FOR TOTAL DATASET SIZE ===
def get_positive_int(prompt, default):
    try:
        value = int(input(f"{prompt} (default {default}): ") or default)
        return max(0, value)
    except ValueError:
        return default

TOTAL_LIMIT = get_positive_int("Enter total number of emails to load", 1000)
PREVIEW_LIMIT = 20  # Emails to preview in the UI

SPAM_FOLDER = 'spam'
HAM_FOLDER = 'easy_ham'

# === LOAD EMAILS FROM FOLDERS ===
def load_emails(folder_path):
    emails = []
    files = os.listdir(folder_path)
    for file in files:
        with open(os.path.join(folder_path, file), 'r', encoding='latin1') as f:
            emails.append(f.read())
    return emails

# Load all available emails first
all_spam = load_emails(SPAM_FOLDER)
all_ham = load_emails(HAM_FOLDER)

# Calculate how many spam and ham to sample for 40% spam and 60% ham
spam_count = int(TOTAL_LIMIT * 0.40)
ham_count = TOTAL_LIMIT - spam_count

# Shuffle the spam and ham emails independently
random.shuffle(all_spam)
random.shuffle(all_ham)

# Sample the required number from each class (handle if dataset has fewer emails)
sampled_spam = all_spam[:spam_count] if len(all_spam) >= spam_count else all_spam
sampled_ham = all_ham[:ham_count] if len(all_ham) >= ham_count else all_ham

# Combine sampled spam and ham and create labels
sampled_emails = sampled_spam + sampled_ham
sampled_labels = [1]*len(sampled_spam) + [0]*len(sampled_ham)

# Shuffle combined sampled data so spam and ham are mixed
combined = list(zip(sampled_emails, sampled_labels))
random.shuffle(combined)
sampled_emails, sampled_labels = zip(*combined)

# === MODEL TRAINING ===
model = make_pipeline(TfidfVectorizer(stop_words='english'), MultinomialNB())
model.fit(sampled_emails, sampled_labels)

# === PREDICTION & METRICS ===
predictions = model.predict(sampled_emails)
probs = model.predict_proba(sampled_emails)[:, 1]

accuracy = accuracy_score(sampled_labels, predictions)
precision = precision_score(sampled_labels, predictions)
recall = recall_score(sampled_labels, predictions)
f1 = f1_score(sampled_labels, predictions)
tn, fp, fn, tp = confusion_matrix(sampled_labels, predictions).ravel()

# === UI FUNCTIONS ===
def get_terminal_width(default=60):
    try:
        return shutil.get_terminal_size().columns - 4
    except:
        return default

def print_email_ui(i, email_text, is_spam, prob):
    width = min(get_terminal_width(), 100)
    border = WHITE + "=" * width + RESET
    icon = "ðŸš« Spam" if is_spam else "ðŸ“¥ Inbox"
    color = RED if is_spam else GREEN
    wrapped_body = textwrap.fill(email_text[:500], width=width)

    print(border)
    print(f"{color}{BOLD}{icon} â€” Email #{i}{RESET}")
    print("-" * width)
    print(wrapped_body + ("\n..." if len(email_text) > 500 else "\n"))
    print(f"{YELLOW}ðŸ“Š Spam Probability:{RESET} {prob:.2%}")
    print(f"{WHITE}ðŸ“Œ Options: {'[Delete] [Move to Inbox]' if is_spam else '[Reply] [Move to Spam]'}{RESET}")
    print(border)

# === CLASSIFY AND DISPLAY PREVIEW ===
inbox = []
spam = []

for i, (email, label) in enumerate(zip(sampled_emails, predictions)):
    is_spam = label == 1
    prob = probs[i]
    (spam if is_spam else inbox).append(email)

    if i < PREVIEW_LIMIT:
        print_email_ui(i + 1, email, is_spam, prob)

# === SUMMARY ===
final_spam_count = sampled_labels.count(1)
final_ham_count = sampled_labels.count(0)

print(f"\n{BOLD}ðŸ“ Folder Summary:{RESET}")
print(f"  ðŸ“¥ Inbox: {len(inbox)}")
print(f"  ðŸš« Spam: {len(spam)}")
print(f"  ðŸ“¦ Total emails used: {len(sampled_emails)} (Spam: {final_spam_count}, Ham: {final_ham_count})\n")

print(f"{BOLD}ðŸ“Š Model Evaluation Metrics:{RESET}")
print(f"  âœ… Accuracy : {accuracy:.2%}")
print(f"  ðŸ“Œ Precision: {precision:.2%}")
print(f"  ðŸ” Recall   : {recall:.2%}")
print(f"  ðŸ§® F1 Score : {f1:.2%}")
print(f"  ðŸ§¾ Confusion Matrix: TP={tp}, FP={fp}, TN={tn}, FN={fn}\n")
